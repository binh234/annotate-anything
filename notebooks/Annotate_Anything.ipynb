{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/binh234/annotate-anything/blob/main/notebooks/Annotate_Anything.ipynb)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "GvhZkQF-LrWY"
      },
      "source": [
        "# Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78Vphb19mDS6"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/binh234/annotate-anything.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "I5ltbBdSmNUT",
        "outputId": "2fd817c9-971c-4160-93cf-3a57c8aa74d2"
      },
      "outputs": [],
      "source": [
        "%cd annotate-anything\n",
        "!pip install -q -r requirements.txt"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "7OpoNiQoLuY3"
      },
      "source": [
        "# Load models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Y4gXYoEnuMYg"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import sys\n",
        "import tempfile\n",
        "import torch\n",
        "\n",
        "import gradio as gr\n",
        "import numpy as np\n",
        "import supervision as sv\n",
        "from PIL import Image\n",
        "from segment_anything import build_sam\n",
        "from segment_anything import SamAutomaticMaskGenerator\n",
        "from segment_anything import SamPredictor\n",
        "from supervision.detection.utils import mask_to_polygons\n",
        "from supervision.detection.utils import xywh_to_xyxy\n",
        "\n",
        "from groundingdino.util.inference import Model as DinoModel\n",
        "\n",
        "sys.path.append(\"tag2text\")\n",
        "\n",
        "from tag2text.models import tag2text\n",
        "from config import *\n",
        "from utils import download_file_hf, detect, segment, show_anns_sam, show_anns_sv, generate_tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "AbBtq48huG-Z"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(abs_weight_dir):\n",
        "    os.makedirs(abs_weight_dir, exist_ok=True)\n",
        "\n",
        "sam_checkpoint = os.path.join(abs_weight_dir, sam_dict[default_sam][\"checkpoint_file\"])\n",
        "if not os.path.exists(sam_checkpoint):\n",
        "    os.system(f\"wget {sam_dict[default_sam]['checkpoint_url']} -O {sam_checkpoint}\")\n",
        "\n",
        "tag2text_checkpoint = os.path.join(abs_weight_dir, tag2text_dict[default_tag2text][\"checkpoint_file\"])\n",
        "if not os.path.exists(tag2text_checkpoint):\n",
        "    os.system(f\"wget {tag2text_dict[default_tag2text]['checkpoint_url']} -O {tag2text_checkpoint}\")\n",
        "\n",
        "dino_checkpoint = os.path.join(abs_weight_dir, dino_dict[default_dino][\"checkpoint_file\"])\n",
        "dino_config_file = os.path.join(abs_weight_dir, dino_dict[default_dino][\"config_file\"])\n",
        "if not os.path.exists(dino_checkpoint):\n",
        "    dino_repo_id = dino_dict[default_dino][\"repo_id\"]\n",
        "    download_file_hf(repo_id=dino_repo_id, filename=dino_dict[default_dino][\"config_file\"], cache_dir=weight_dir)\n",
        "    download_file_hf(repo_id=dino_repo_id, filename=dino_dict[default_dino][\"checkpoint_file\"], cache_dir=weight_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhCaI9zyuQD4",
        "outputId": "c1eca527-344f-4911-cbcd-5fb2d1bb4899"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/encoder/layer/0/crossattention/self/query is tied\n",
            "/encoder/layer/0/crossattention/self/key is tied\n",
            "/encoder/layer/0/crossattention/self/value is tied\n",
            "/encoder/layer/0/crossattention/output/dense is tied\n",
            "/encoder/layer/0/crossattention/output/LayerNorm is tied\n",
            "/encoder/layer/0/intermediate/dense is tied\n",
            "/encoder/layer/0/output/dense is tied\n",
            "/encoder/layer/0/output/LayerNorm is tied\n",
            "/encoder/layer/1/crossattention/self/query is tied\n",
            "/encoder/layer/1/crossattention/self/key is tied\n",
            "/encoder/layer/1/crossattention/self/value is tied\n",
            "/encoder/layer/1/crossattention/output/dense is tied\n",
            "/encoder/layer/1/crossattention/output/LayerNorm is tied\n",
            "/encoder/layer/1/intermediate/dense is tied\n",
            "/encoder/layer/1/output/dense is tied\n",
            "/encoder/layer/1/output/LayerNorm is tied\n",
            "--------------\n",
            "/content/weights/tag2text_swin_14m.pth\n",
            "--------------\n",
            "load checkpoint from /content/weights/tag2text_swin_14m.pth\n",
            "vit: swin_b\n",
            "msg _IncompatibleKeys(missing_keys=['visual_encoder.layers.0.blocks.0.attn.relative_position_index', 'visual_encoder.layers.0.blocks.1.attn_mask', 'visual_encoder.layers.0.blocks.1.attn.relative_position_index', 'visual_encoder.layers.1.blocks.0.attn.relative_position_index', 'visual_encoder.layers.1.blocks.1.attn_mask', 'visual_encoder.layers.1.blocks.1.attn.relative_position_index', 'visual_encoder.layers.2.blocks.0.attn.relative_position_index', 'visual_encoder.layers.2.blocks.1.attn_mask', 'visual_encoder.layers.2.blocks.1.attn.relative_position_index', 'visual_encoder.layers.2.blocks.2.attn.relative_position_index', 'visual_encoder.layers.2.blocks.3.attn_mask', 'visual_encoder.layers.2.blocks.3.attn.relative_position_index', 'visual_encoder.layers.2.blocks.4.attn.relative_position_index', 'visual_encoder.layers.2.blocks.5.attn_mask', 'visual_encoder.layers.2.blocks.5.attn.relative_position_index', 'visual_encoder.layers.2.blocks.6.attn.relative_position_index', 'visual_encoder.layers.2.blocks.7.attn_mask', 'visual_encoder.layers.2.blocks.7.attn.relative_position_index', 'visual_encoder.layers.2.blocks.8.attn.relative_position_index', 'visual_encoder.layers.2.blocks.9.attn_mask', 'visual_encoder.layers.2.blocks.9.attn.relative_position_index', 'visual_encoder.layers.2.blocks.10.attn.relative_position_index', 'visual_encoder.layers.2.blocks.11.attn_mask', 'visual_encoder.layers.2.blocks.11.attn.relative_position_index', 'visual_encoder.layers.2.blocks.12.attn.relative_position_index', 'visual_encoder.layers.2.blocks.13.attn_mask', 'visual_encoder.layers.2.blocks.13.attn.relative_position_index', 'visual_encoder.layers.2.blocks.14.attn.relative_position_index', 'visual_encoder.layers.2.blocks.15.attn_mask', 'visual_encoder.layers.2.blocks.15.attn.relative_position_index', 'visual_encoder.layers.2.blocks.16.attn.relative_position_index', 'visual_encoder.layers.2.blocks.17.attn_mask', 'visual_encoder.layers.2.blocks.17.attn.relative_position_index', 'visual_encoder.layers.3.blocks.0.attn.relative_position_index', 'visual_encoder.layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=[])\n",
            "final text_encoder_type: bert-base-uncased\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "# load model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "tag2text_model = tag2text.tag2text_caption(\n",
        "    pretrained=tag2text_checkpoint,\n",
        "    image_size=384,\n",
        "    vit=\"swin_b\",\n",
        "    delete_tag_index=delete_tag_index,\n",
        ")\n",
        "# threshold for tagging\n",
        "# we reduce the threshold to obtain more tags\n",
        "tag2text_model.threshold = 0.64\n",
        "tag2text_model.to(device)\n",
        "tag2text_model.eval()\n",
        "\n",
        "\n",
        "sam = build_sam(checkpoint=sam_checkpoint)\n",
        "sam.to(device=device)\n",
        "sam_predictor = SamPredictor(sam)\n",
        "sam_automask_generator = SamAutomaticMaskGenerator(sam)\n",
        "\n",
        "grounding_dino_model = DinoModel(\n",
        "    model_config_path=dino_config_file, model_checkpoint_path=dino_checkpoint\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BKw9imiOLwr-"
      },
      "source": [
        "# Gradio App"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 854
        },
        "id": "WGyHgVVZmSYr",
        "outputId": "b4cc50c3-a781-4765-add3-7cd110b788da"
      },
      "outputs": [],
      "source": [
        "def process(image_path, task, prompt, box_threshold, text_threshold, iou_threshold):\n",
        "    global tag2text_model, sam_predictor, sam_automask_generator, grounding_dino_model, device\n",
        "    output_gallery = []\n",
        "    detections = None\n",
        "    metadata = {\"image\": {}, \"annotations\": []}\n",
        "\n",
        "    try:\n",
        "        # Load image\n",
        "        image = Image.open(image_path)\n",
        "        image_pil = image.convert(\"RGB\")\n",
        "        image = np.array(image_pil)\n",
        "\n",
        "        # Extract image metadata\n",
        "        filename = os.path.basename(image_path)\n",
        "        h, w = image.shape[:2]\n",
        "        metadata[\"image\"][\"file_name\"] = filename\n",
        "        metadata[\"image\"][\"width\"] = w\n",
        "        metadata[\"image\"][\"height\"] = h\n",
        "\n",
        "        # Generate tags\n",
        "        if task in [\"auto\", \"detection\"] and prompt == \"\":\n",
        "            tags, caption = generate_tags(tag2text_model, image_pil, \"None\", device)\n",
        "            prompt = \" . \".join(tags)\n",
        "            print(f\"Caption: {caption}\")\n",
        "            print(f\"Tags: {tags}\")\n",
        "\n",
        "            # ToDo: Extract metadata\n",
        "            metadata[\"image\"][\"caption\"] = caption\n",
        "            metadata[\"image\"][\"tags\"] = tags\n",
        "\n",
        "        if prompt:\n",
        "            metadata[\"prompt\"] = prompt\n",
        "            print(f\"Prompt: {prompt}\")\n",
        "\n",
        "        # Detect boxes\n",
        "        if prompt != \"\":\n",
        "            detections, phrases, classes = detect(\n",
        "                grounding_dino_model,\n",
        "                image,\n",
        "                caption=prompt,\n",
        "                box_threshold=box_threshold,\n",
        "                text_threshold=text_threshold,\n",
        "                iou_threshold=iou_threshold,\n",
        "                post_process=True,\n",
        "            )\n",
        "\n",
        "            # Draw boxes\n",
        "            box_annotator = sv.BoxAnnotator()\n",
        "            labels = [\n",
        "                f\"{classes[class_id] if class_id else 'Unkown'} {confidence:0.2f}\"\n",
        "                for _, _, confidence, class_id, _ in detections\n",
        "            ]\n",
        "            image = box_annotator.annotate(\n",
        "                scene=image, detections=detections, labels=labels\n",
        "            )\n",
        "            output_gallery.append(image)\n",
        "\n",
        "        # Segmentation\n",
        "        if task in [\"auto\", \"segment\"]:\n",
        "            if detections:\n",
        "                masks, scores = segment(\n",
        "                    sam_predictor, image=image, boxes=detections.xyxy\n",
        "                )\n",
        "                detections.mask = masks\n",
        "            else:\n",
        "                masks = sam_automask_generator.generate(image)\n",
        "                sorted_generated_masks = sorted(\n",
        "                    masks, key=lambda x: x[\"area\"], reverse=True\n",
        "                )\n",
        "\n",
        "                xywh = np.array([mask[\"bbox\"] for mask in sorted_generated_masks])\n",
        "                mask = np.array(\n",
        "                    [mask[\"segmentation\"] for mask in sorted_generated_masks]\n",
        "                )\n",
        "                scores = np.array(\n",
        "                    [mask[\"predicted_iou\"] for mask in sorted_generated_masks]\n",
        "                )\n",
        "                detections = sv.Detections(\n",
        "                    xyxy=xywh_to_xyxy(boxes_xywh=xywh), mask=mask\n",
        "                )\n",
        "                # opacity = 0.4\n",
        "                # mask_image, _ = show_anns(masks)\n",
        "                # annotated_image = np.uint8(mask_image * opacity + image * (1 - opacity))\n",
        "            \n",
        "            mask_annotator = sv.MaskAnnotator()\n",
        "            mask_image = np.zeros_like(image, dtype=np.uint8)\n",
        "            mask_image = mask_annotator.annotate(\n",
        "                mask_image, detections=detections, opacity=1\n",
        "            )\n",
        "            annotated_image = mask_annotator.annotate(image, detections=detections)\n",
        "            output_gallery.append(mask_image)\n",
        "            output_gallery.append(annotated_image)\n",
        "\n",
        "        # ToDo: Extract metadata\n",
        "        if detections:\n",
        "            id = 1\n",
        "            for (xyxy, mask, confidence, class_id, _), area, box_area, score in zip(\n",
        "                detections, detections.area, detections.box_area, scores\n",
        "            ):\n",
        "                annotation = {\n",
        "                    \"id\": id,\n",
        "                    \"bbox\": [int(x) for x in xyxy],\n",
        "                    \"box_area\": float(box_area),\n",
        "                }\n",
        "                if class_id:\n",
        "                    annotation[\"box_confidence\"] = float(confidence)\n",
        "                    annotation[\"label\"] = classes[class_id] if class_id else \"Unkown\"\n",
        "                if mask is not None:\n",
        "                    # annotation[\"segmentation\"] = mask_to_polygons(mask)\n",
        "                    annotation[\"area\"] = int(area)\n",
        "                    annotation[\"predicted_iou\"] = float(score)\n",
        "                metadata[\"annotations\"].append(annotation)\n",
        "                id += 1\n",
        "\n",
        "\n",
        "        meta_file = tempfile.NamedTemporaryFile(delete=False, suffix=\".json\")\n",
        "        meta_file_path = meta_file.name\n",
        "        with open(meta_file_path, \"w\", encoding=\"utf-8\") as fp:\n",
        "            json.dump(metadata, fp)\n",
        "\n",
        "        return output_gallery, meta_file_path\n",
        "    except Exception as error:\n",
        "        raise gr.Error(f\"global exception: {error}\")\n",
        "\n",
        "\n",
        "title = \"Annotate Anything\"\n",
        "\n",
        "with gr.Blocks(css=\"style.css\", title=title) as demo:\n",
        "    with gr.Row(elem_classes=[\"container\"]):\n",
        "        with gr.Column(scale=1):\n",
        "            input_image = gr.Image(type=\"filepath\", label=\"Input\")\n",
        "            task = gr.Dropdown(\n",
        "                [\"detect\", \"segment\", \"auto\"], value=\"auto\", label=\"task_type\"\n",
        "            )\n",
        "            text_prompt = gr.Textbox(label=\"Detection Prompt\")\n",
        "            with gr.Accordion(\"Advanced parameters\", open=False):\n",
        "                box_threshold = gr.Slider(\n",
        "                    minimum=0,\n",
        "                    maximum=1,\n",
        "                    value=0.3,\n",
        "                    step=0.05,\n",
        "                    label=\"Box threshold\",\n",
        "                    info=\"Hash size to use for image hashing\",\n",
        "                )\n",
        "                text_threshold = gr.Slider(\n",
        "                    minimum=0,\n",
        "                    maximum=1,\n",
        "                    value=0.25,\n",
        "                    step=0.05,\n",
        "                    label=\"Text threshold\",\n",
        "                    info=\"Number of history images used to find out duplicate image\",\n",
        "                )\n",
        "                iou_threshold = gr.Slider(\n",
        "                    minimum=0,\n",
        "                    maximum=1,\n",
        "                    value=0.5,\n",
        "                    step=0.05,\n",
        "                    label=\"IOU threshold\",\n",
        "                    info=\"Minimum similarity threshold (in percent) to consider 2 images to be similar\",\n",
        "                )\n",
        "            run_button = gr.Button(label=\"Run\")\n",
        "\n",
        "        with gr.Column(scale=2):\n",
        "            gallery = gr.Gallery(\n",
        "                label=\"Generated images\", show_label=False, elem_id=\"gallery\"\n",
        "            ).style(preview=True, grid=2, object_fit=\"scale-down\")\n",
        "            meta_file = gr.File(label=\"Metadata file\")\n",
        "    with gr.Row(elem_classes=[\"container\"]):\n",
        "        gr.Examples(\n",
        "            [\n",
        "                [\"examples/dog.png\", \"auto\", \"\"],\n",
        "                [\"examples/eiffel.jpg\", \"auto\", \"\"],\n",
        "                [\"examples/eiffel.png\", \"segment\", \"\"],\n",
        "                [\"examples/girl.png\", \"auto\", \"girl . face\"],\n",
        "                [\"examples/horse.png\", \"detect\", \"horse\"],\n",
        "                [\"examples/horses.jpg\", \"auto\", \"horse\"],\n",
        "                [\"examples/traffic.jpg\", \"auto\", \"\"],\n",
        "            ],\n",
        "            [input_image, task, text_prompt],\n",
        "        )\n",
        "    run_button.click(\n",
        "        fn=process,\n",
        "        inputs=[\n",
        "            input_image,\n",
        "            task,\n",
        "            text_prompt,\n",
        "            box_threshold,\n",
        "            text_threshold,\n",
        "            iou_threshold,\n",
        "        ],\n",
        "        outputs=[gallery, meta_file],\n",
        "    )\n",
        "\n",
        "demo.queue(concurrency_count=2).launch(debug=True, share=True)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TUaTtY0BLzMu"
      },
      "source": [
        "# Command lines"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "CVR9v70NJYKR"
      },
      "source": [
        "The metadata file will contain the following information:\n",
        "\n",
        "```text\n",
        "{\n",
        "    \"image\"                 : image_info,\n",
        "    \"annotations\"           : [annotation],\n",
        "}\n",
        "\n",
        "image_info {\n",
        "    \"width\"                 : int,              # Image width\n",
        "    \"height\"                : int,              # Image height\n",
        "    \"file_name\"             : str,              # Image filename\n",
        "    \"caption\"               : str,              # Image caption\n",
        "    \"tags\"                  : [str],            # Image tags\n",
        "}\n",
        "\n",
        "annotation {\n",
        "    \"id\"                    : int,              # Annotation id\n",
        "    \"bbox\"                  : [x1, y1, x2, y2],     # The box around the mask, in XYXY format\n",
        "    \"area\"                  : int,              # The area in pixels of the mask\n",
        "    \"box_area\"              : float,            # The area in pixels of the bounding box\n",
        "    \"predicted_iou\"         : float,            # The model's own prediction of the mask's quality\n",
        "    \"box_confidence\"        : float,            # A measure of the box's quality\n",
        "    \"label\"                 : str,              # Predicted class for the object inside the bounding box (if exist)\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "goZ_OqwFwfGi",
        "outputId": "173daf9f-9e2f-4504-95ac-599a5418fc52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-06-04 15:56:47.478216: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Loading SAM...\n",
            "7it [02:08, 18.31s/it, Processing examples/traffic.jpg]\n"
          ]
        }
      ],
      "source": [
        "!python annotate_anything.py -i examples -o /content/outputs_segment --task segment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xgl-pBJB2wJh"
      },
      "outputs": [],
      "source": [
        "!python annotate_anything.py -i examples -o /content/outputs_auto --task auto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZAvbh66EdLF"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
