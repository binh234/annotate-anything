{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/binh234/annotate-anything/blob/main/notebooks/Annotate_Anything.ipynb)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "GvhZkQF-LrWY"
      },
      "source": [
        "# Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78Vphb19mDS6"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/binh234/annotate-anything.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "I5ltbBdSmNUT",
        "outputId": "2fd817c9-971c-4160-93cf-3a57c8aa74d2"
      },
      "outputs": [],
      "source": [
        "%cd annotate-anything\n",
        "!pip install -q -r requirements.txt"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "7OpoNiQoLuY3"
      },
      "source": [
        "# Load models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Y4gXYoEnuMYg"
      },
      "outputs": [],
      "source": [
        "import functools\n",
        "import json\n",
        "import os\n",
        "import sys\n",
        "import tempfile\n",
        "import torch\n",
        "\n",
        "import cv2\n",
        "import gradio as gr\n",
        "import numpy as np\n",
        "import supervision as sv\n",
        "from PIL import Image\n",
        "from segment_anything import build_sam\n",
        "from segment_anything import SamAutomaticMaskGenerator\n",
        "from segment_anything import SamPredictor\n",
        "from supervision.detection.utils import mask_to_polygons\n",
        "from supervision.detection.utils import xywh_to_xyxy\n",
        "\n",
        "from groundingdino.util.inference import Model as DinoModel\n",
        "\n",
        "sys.path.append(\"tag2text\")\n",
        "\n",
        "from tag2text.models import tag2text\n",
        "from config import *\n",
        "from utils import download_file_hf, detect, segment, show_anns_sam, show_anns_sv, generate_tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "AbBtq48huG-Z"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(abs_weight_dir):\n",
        "    os.makedirs(abs_weight_dir, exist_ok=True)\n",
        "\n",
        "sam_checkpoint = os.path.join(abs_weight_dir, sam_dict[default_sam][\"checkpoint_file\"])\n",
        "if not os.path.exists(sam_checkpoint):\n",
        "    os.system(f\"wget {sam_dict[default_sam]['checkpoint_url']} -O {sam_checkpoint}\")\n",
        "\n",
        "tag2text_checkpoint = os.path.join(abs_weight_dir, tag2text_dict[default_tag2text][\"checkpoint_file\"])\n",
        "if not os.path.exists(tag2text_checkpoint):\n",
        "    os.system(f\"wget {tag2text_dict[default_tag2text]['checkpoint_url']} -O {tag2text_checkpoint}\")\n",
        "\n",
        "dino_checkpoint = os.path.join(abs_weight_dir, dino_dict[default_dino][\"checkpoint_file\"])\n",
        "dino_config_file = os.path.join(abs_weight_dir, dino_dict[default_dino][\"config_file\"])\n",
        "if not os.path.exists(dino_checkpoint):\n",
        "    dino_repo_id = dino_dict[default_dino][\"repo_id\"]\n",
        "    download_file_hf(repo_id=dino_repo_id, filename=dino_dict[default_dino][\"config_file\"], cache_dir=weight_dir)\n",
        "    download_file_hf(repo_id=dino_repo_id, filename=dino_dict[default_dino][\"checkpoint_file\"], cache_dir=weight_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhCaI9zyuQD4",
        "outputId": "c1eca527-344f-4911-cbcd-5fb2d1bb4899"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/encoder/layer/0/crossattention/self/query is tied\n",
            "/encoder/layer/0/crossattention/self/key is tied\n",
            "/encoder/layer/0/crossattention/self/value is tied\n",
            "/encoder/layer/0/crossattention/output/dense is tied\n",
            "/encoder/layer/0/crossattention/output/LayerNorm is tied\n",
            "/encoder/layer/0/intermediate/dense is tied\n",
            "/encoder/layer/0/output/dense is tied\n",
            "/encoder/layer/0/output/LayerNorm is tied\n",
            "/encoder/layer/1/crossattention/self/query is tied\n",
            "/encoder/layer/1/crossattention/self/key is tied\n",
            "/encoder/layer/1/crossattention/self/value is tied\n",
            "/encoder/layer/1/crossattention/output/dense is tied\n",
            "/encoder/layer/1/crossattention/output/LayerNorm is tied\n",
            "/encoder/layer/1/intermediate/dense is tied\n",
            "/encoder/layer/1/output/dense is tied\n",
            "/encoder/layer/1/output/LayerNorm is tied\n",
            "--------------\n",
            "/content/weights/tag2text_swin_14m.pth\n",
            "--------------\n",
            "load checkpoint from /content/weights/tag2text_swin_14m.pth\n",
            "vit: swin_b\n",
            "msg _IncompatibleKeys(missing_keys=['visual_encoder.layers.0.blocks.0.attn.relative_position_index', 'visual_encoder.layers.0.blocks.1.attn_mask', 'visual_encoder.layers.0.blocks.1.attn.relative_position_index', 'visual_encoder.layers.1.blocks.0.attn.relative_position_index', 'visual_encoder.layers.1.blocks.1.attn_mask', 'visual_encoder.layers.1.blocks.1.attn.relative_position_index', 'visual_encoder.layers.2.blocks.0.attn.relative_position_index', 'visual_encoder.layers.2.blocks.1.attn_mask', 'visual_encoder.layers.2.blocks.1.attn.relative_position_index', 'visual_encoder.layers.2.blocks.2.attn.relative_position_index', 'visual_encoder.layers.2.blocks.3.attn_mask', 'visual_encoder.layers.2.blocks.3.attn.relative_position_index', 'visual_encoder.layers.2.blocks.4.attn.relative_position_index', 'visual_encoder.layers.2.blocks.5.attn_mask', 'visual_encoder.layers.2.blocks.5.attn.relative_position_index', 'visual_encoder.layers.2.blocks.6.attn.relative_position_index', 'visual_encoder.layers.2.blocks.7.attn_mask', 'visual_encoder.layers.2.blocks.7.attn.relative_position_index', 'visual_encoder.layers.2.blocks.8.attn.relative_position_index', 'visual_encoder.layers.2.blocks.9.attn_mask', 'visual_encoder.layers.2.blocks.9.attn.relative_position_index', 'visual_encoder.layers.2.blocks.10.attn.relative_position_index', 'visual_encoder.layers.2.blocks.11.attn_mask', 'visual_encoder.layers.2.blocks.11.attn.relative_position_index', 'visual_encoder.layers.2.blocks.12.attn.relative_position_index', 'visual_encoder.layers.2.blocks.13.attn_mask', 'visual_encoder.layers.2.blocks.13.attn.relative_position_index', 'visual_encoder.layers.2.blocks.14.attn.relative_position_index', 'visual_encoder.layers.2.blocks.15.attn_mask', 'visual_encoder.layers.2.blocks.15.attn.relative_position_index', 'visual_encoder.layers.2.blocks.16.attn.relative_position_index', 'visual_encoder.layers.2.blocks.17.attn_mask', 'visual_encoder.layers.2.blocks.17.attn.relative_position_index', 'visual_encoder.layers.3.blocks.0.attn.relative_position_index', 'visual_encoder.layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=[])\n",
            "final text_encoder_type: bert-base-uncased\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "# load model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "tag2text_model = tag2text.tag2text_caption(\n",
        "    pretrained=tag2text_checkpoint,\n",
        "    image_size=384,\n",
        "    vit=\"swin_b\",\n",
        "    delete_tag_index=delete_tag_index,\n",
        ")\n",
        "# threshold for tagging\n",
        "# we reduce the threshold to obtain more tags\n",
        "tag2text_model.threshold = 0.64\n",
        "tag2text_model.to(device)\n",
        "tag2text_model.eval()\n",
        "\n",
        "\n",
        "sam = build_sam(checkpoint=sam_checkpoint)\n",
        "sam.to(device=device)\n",
        "sam_predictor = SamPredictor(sam)\n",
        "sam_automask_generator = SamAutomaticMaskGenerator(sam)\n",
        "\n",
        "grounding_dino_model = DinoModel(\n",
        "    model_config_path=dino_config_file, model_checkpoint_path=dino_checkpoint\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BKw9imiOLwr-"
      },
      "source": [
        "# Gradio App"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 854
        },
        "id": "WGyHgVVZmSYr",
        "outputId": "b4cc50c3-a781-4765-add3-7cd110b788da"
      },
      "outputs": [],
      "source": [
        "def process(\n",
        "    image_path,\n",
        "    task,\n",
        "    prompt,\n",
        "    box_threshold,\n",
        "    text_threshold,\n",
        "    iou_threshold,\n",
        "    kernel_size,\n",
        "    expand_mask,\n",
        "):\n",
        "    global tag2text_model, sam_predictor, sam_automask_generator, grounding_dino_model, device\n",
        "    output_gallery = []\n",
        "    detections = None\n",
        "    metadata = {\"image\": {}, \"annotations\": []}\n",
        "\n",
        "    try:\n",
        "        # Load image\n",
        "        image = Image.open(image_path)\n",
        "        image_pil = image.convert(\"RGB\")\n",
        "        image = np.array(image_pil)\n",
        "        orig_image = image.copy()\n",
        "\n",
        "        # Extract image metadata\n",
        "        filename = os.path.basename(image_path)\n",
        "        h, w = image.shape[:2]\n",
        "        metadata[\"image\"][\"file_name\"] = filename\n",
        "        metadata[\"image\"][\"width\"] = w\n",
        "        metadata[\"image\"][\"height\"] = h\n",
        "\n",
        "        # Generate tags\n",
        "        if task in [\"auto\", \"detection\"] and prompt == \"\":\n",
        "            tags, caption = generate_tags(tag2text_model, image_pil, \"None\", device)\n",
        "            prompt = \" . \".join(tags)\n",
        "            print(f\"Caption: {caption}\")\n",
        "            print(f\"Tags: {tags}\")\n",
        "\n",
        "            # ToDo: Extract metadata\n",
        "            metadata[\"image\"][\"caption\"] = caption\n",
        "            metadata[\"image\"][\"tags\"] = tags\n",
        "\n",
        "        if prompt:\n",
        "            metadata[\"prompt\"] = prompt\n",
        "            print(f\"Prompt: {prompt}\")\n",
        "\n",
        "        # Detect boxes\n",
        "        if prompt != \"\":\n",
        "            detections, phrases, classes = detect(\n",
        "                grounding_dino_model,\n",
        "                image,\n",
        "                caption=prompt,\n",
        "                box_threshold=box_threshold,\n",
        "                text_threshold=text_threshold,\n",
        "                iou_threshold=iou_threshold,\n",
        "                post_process=True,\n",
        "            )\n",
        "            print(phrases)\n",
        "\n",
        "            # Draw boxes\n",
        "            box_annotator = sv.BoxAnnotator()\n",
        "            labels = [\n",
        "                f\"{phrases[i]} {detections.confidence[i]:0.2f}\"\n",
        "                for i in range(len(phrases))\n",
        "            ]\n",
        "            image = box_annotator.annotate(\n",
        "                scene=image, detections=detections, labels=labels\n",
        "            )\n",
        "            output_gallery.append(image)\n",
        "\n",
        "        # Segmentation\n",
        "        if task in [\"auto\", \"segment\"]:\n",
        "            kernel = cv2.getStructuringElement(\n",
        "                cv2.MORPH_ELLIPSE, (2 * kernel_size + 1, 2 * kernel_size + 1)\n",
        "            )\n",
        "            if detections:\n",
        "                masks, scores = segment(\n",
        "                    sam_predictor, image=orig_image, boxes=detections.xyxy\n",
        "                )\n",
        "                if expand_mask:\n",
        "                    masks = [\n",
        "                        cv2.dilate(mask.astype(np.uint8), kernel) for mask in masks\n",
        "                    ]\n",
        "                else:\n",
        "                    masks = [\n",
        "                        cv2.morphologyEx(mask.astype(np.uint8), cv2.MORPH_CLOSE, kernel)\n",
        "                        for mask in masks\n",
        "                    ]\n",
        "                detections.mask = masks\n",
        "                binary_mask = functools.reduce(\n",
        "                    lambda x, y: x + y, detections.mask\n",
        "                ).astype(bool)\n",
        "            else:\n",
        "                masks = sam_automask_generator.generate(orig_image)\n",
        "                sorted_generated_masks = sorted(\n",
        "                    masks, key=lambda x: x[\"area\"], reverse=True\n",
        "                )\n",
        "\n",
        "                xywh = np.array([mask[\"bbox\"] for mask in sorted_generated_masks])\n",
        "                scores = np.array(\n",
        "                    [mask[\"predicted_iou\"] for mask in sorted_generated_masks]\n",
        "                )\n",
        "                if expand_mask:\n",
        "                    mask = np.array(\n",
        "                        [\n",
        "                            cv2.dilate(mask[\"segmentation\"].astype(np.uint8), kernel)\n",
        "                            for mask in sorted_generated_masks\n",
        "                        ]\n",
        "                    )\n",
        "                else:\n",
        "                    mask = np.array(\n",
        "                        [mask[\"segmentation\"] for mask in sorted_generated_masks]\n",
        "                    )\n",
        "                detections = sv.Detections(\n",
        "                    xyxy=xywh_to_xyxy(boxes_xywh=xywh), mask=mask\n",
        "                )\n",
        "                binary_mask = None\n",
        "\n",
        "            mask_annotator = sv.MaskAnnotator()\n",
        "            mask_image = np.zeros_like(image, dtype=np.uint8)\n",
        "            mask_image = mask_annotator.annotate(\n",
        "                mask_image, detections=detections, opacity=1\n",
        "            )\n",
        "            annotated_image = mask_annotator.annotate(image, detections=detections)\n",
        "\n",
        "            output_gallery.append(mask_image)\n",
        "            if binary_mask is not None:\n",
        "                binary_mask_image = binary_mask * 255\n",
        "                cutout_image = np.expand_dims(binary_mask, axis=-1) * orig_image\n",
        "                output_gallery.append(binary_mask_image)\n",
        "                output_gallery.append(cutout_image)\n",
        "            output_gallery.append(annotated_image)\n",
        "\n",
        "        # ToDo: Extract metadata\n",
        "        if detections:\n",
        "            i = 0\n",
        "            for (xyxy, mask, confidence, _, _), area, box_area in zip(\n",
        "                detections, detections.area, detections.box_area\n",
        "            ):\n",
        "                annotation = {\n",
        "                    \"id\": i + 1,\n",
        "                    \"bbox\": [int(x) for x in xyxy],\n",
        "                    \"box_area\": float(box_area),\n",
        "                }\n",
        "                if confidence:\n",
        "                    annotation[\"confidence\"] = float(confidence)\n",
        "                    annotation[\"label\"] = phrases[i]\n",
        "                if mask is not None:\n",
        "                    # annotation[\"segmentation\"] = mask_to_polygons(mask)\n",
        "                    annotation[\"area\"] = int(area)\n",
        "                    annotation[\"predicted_iou\"] = float(scores[i])\n",
        "                metadata[\"annotations\"].append(annotation)\n",
        "                i += 1\n",
        "\n",
        "        meta_file = tempfile.NamedTemporaryFile(delete=False, suffix=\".json\")\n",
        "        meta_file_path = meta_file.name\n",
        "        with open(meta_file_path, \"w\", encoding=\"utf-8\") as fp:\n",
        "            json.dump(metadata, fp)\n",
        "\n",
        "        return output_gallery, meta_file_path\n",
        "    except Exception as error:\n",
        "        raise gr.Error(f\"global exception: {error}\")\n",
        "\n",
        "\n",
        "title = \"Annotate Anything\"\n",
        "\n",
        "with gr.Blocks(css=\"style.css\", title=title) as demo:\n",
        "    with gr.Row(elem_classes=[\"container\"]):\n",
        "        with gr.Column(scale=1):\n",
        "            input_image = gr.Image(type=\"filepath\", label=\"Input\")\n",
        "            task = gr.Dropdown(\n",
        "                [\"detect\", \"segment\", \"auto\"], value=\"auto\", label=\"task_type\"\n",
        "            )\n",
        "            text_prompt = gr.Textbox(\n",
        "                label=\"Detection Prompt\",\n",
        "                info=\"To detect multiple objects, seperating each name with '.', like this: cat . dog . chair \",\n",
        "                placeholder=\"cat . dog . chair\",\n",
        "            )\n",
        "            with gr.Accordion(\"Advanced parameters\", open=False):\n",
        "                box_threshold = gr.Slider(\n",
        "                    minimum=0,\n",
        "                    maximum=1,\n",
        "                    value=0.3,\n",
        "                    step=0.05,\n",
        "                    label=\"Box threshold\",\n",
        "                )\n",
        "                text_threshold = gr.Slider(\n",
        "                    minimum=0,\n",
        "                    maximum=1,\n",
        "                    value=0.25,\n",
        "                    step=0.05,\n",
        "                    label=\"Text threshold\",\n",
        "                )\n",
        "                iou_threshold = gr.Slider(\n",
        "                    minimum=0,\n",
        "                    maximum=1,\n",
        "                    value=0.5,\n",
        "                    step=0.05,\n",
        "                    label=\"IOU threshold\",\n",
        "                    info=\"Intersection over Union threshold\",\n",
        "                )\n",
        "                kernel_size = gr.Slider(\n",
        "                    minimum=1,\n",
        "                    maximum=5,\n",
        "                    value=2,\n",
        "                    step=1,\n",
        "                    label=\"Kernel size\",\n",
        "                    info=\"Use to smooth segment masks\",\n",
        "                )\n",
        "                expand_mask = gr.Checkbox(\n",
        "                    label=\"Expand mask\",\n",
        "                )\n",
        "            run_button = gr.Button(label=\"Run\")\n",
        "\n",
        "        with gr.Column(scale=2):\n",
        "            gallery = gr.Gallery(\n",
        "                label=\"Generated images\", show_label=False, elem_id=\"gallery\"\n",
        "            ).style(preview=True, grid=2, object_fit=\"scale-down\")\n",
        "            meta_file = gr.File(label=\"Metadata file\")\n",
        "    with gr.Row(elem_classes=[\"container\"]):\n",
        "        gr.Examples(\n",
        "            [\n",
        "                [\"examples/dog.png\", \"auto\", \"\"],\n",
        "                [\"examples/eiffel.jpg\", \"auto\", \"tower . lake . grass . sky\"],\n",
        "                [\"examples/eiffel.png\", \"segment\", \"\"],\n",
        "                [\"examples/girl.png\", \"auto\", \"girl . face\"],\n",
        "                [\"examples/horse.png\", \"detect\", \"horse\"],\n",
        "                [\"examples/horses.jpg\", \"auto\", \"horse\"],\n",
        "                [\"examples/traffic.jpg\", \"auto\", \"\"],\n",
        "            ],\n",
        "            [input_image, task, text_prompt],\n",
        "        )\n",
        "    run_button.click(\n",
        "        fn=process,\n",
        "        inputs=[\n",
        "            input_image,\n",
        "            task,\n",
        "            text_prompt,\n",
        "            box_threshold,\n",
        "            text_threshold,\n",
        "            iou_threshold,\n",
        "            kernel_size,\n",
        "            expand_mask,\n",
        "        ],\n",
        "        outputs=[gallery, meta_file],\n",
        "    )\n",
        "\n",
        "demo.queue(concurrency_count=2).launch(share=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TUaTtY0BLzMu"
      },
      "source": [
        "# Command lines"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "CVR9v70NJYKR"
      },
      "source": [
        "The metadata file will contain the following information:\n",
        "\n",
        "```text\n",
        "{\n",
        "    \"image\"                 : image_info,\n",
        "    \"annotations\"           : [annotation],\n",
        "}\n",
        "\n",
        "image_info {\n",
        "    \"width\"                 : int,              # Image width\n",
        "    \"height\"                : int,              # Image height\n",
        "    \"file_name\"             : str,              # Image filename\n",
        "    \"caption\"               : str,              # Image caption\n",
        "    \"tags\"                  : [str],            # Image tags\n",
        "}\n",
        "\n",
        "annotation {\n",
        "    \"id\"                    : int,              # Annotation id\n",
        "    \"bbox\"                  : [x1, y1, x2, y2],     # The box around the mask, in XYXY format\n",
        "    \"area\"                  : int,              # The area in pixels of the mask\n",
        "    \"box_area\"              : float,            # The area in pixels of the bounding box\n",
        "    \"predicted_iou\"         : float,            # The model's own prediction of the mask's quality\n",
        "    \"confidence\"            : float,            # A measure of the prediction confidency\n",
        "    \"label\"                 : str,              # Predicted class for the object inside the bounding box (if exist)\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "goZ_OqwFwfGi",
        "outputId": "173daf9f-9e2f-4504-95ac-599a5418fc52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-06-04 15:56:47.478216: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Loading SAM...\n",
            "7it [02:08, 18.31s/it, Processing examples/traffic.jpg]\n"
          ]
        }
      ],
      "source": [
        "!python annotate_anything.py -i examples -o /content/outputs_segment --task segment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xgl-pBJB2wJh"
      },
      "outputs": [],
      "source": [
        "!python annotate_anything.py -i examples -o /content/outputs_auto --task auto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZAvbh66EdLF"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
